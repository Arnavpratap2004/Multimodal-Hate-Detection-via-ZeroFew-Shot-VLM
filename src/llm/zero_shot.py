"""
Zero-shot hate speech classification.

Zero-shot classification uses only the task description without any examples.
This tests the model's inherent understanding of hate speech.
"""

from ..vlm.base import VLMOutput
from .base import BaseClassifier, BaseLLM, ClassificationResult
from .openrouter_llm import OpenRouterLLM


# System prompt for zero-shot classification - RECALL-OPTIMIZED
ZERO_SHOT_SYSTEM_PROMPT = """You are a multimodal hate-speech analysis expert specializing in subtle, implicit, and culturally contextualized hate in internet memes.

You will be given:
1) A detailed image description generated by a Vision-Language Model
2) OCR-extracted text from the image (may include slang, sarcasm, emojis, or code-mixed language)
3) Cultural and contextual analysis

Your task is to classify the content strictly as:
→ HATE
→ NON-HATE

=====================================
CRITICAL CLASSIFICATION INSTRUCTIONS
=====================================

1. PRIORITIZE RECALL OVER CONSERVATISM
   - If content *implicitly*, *indirectly*, or *contextually* attacks a protected group or individual, classify as HATE.
   - When uncertain between HATE and NON-HATE, choose HATE **if reasonable contextual harm exists**.

2. EXPAND THE DEFINITION OF HATE
   Consider content as HATE if it includes ANY of the following:
   - Dehumanization, ridicule, or humiliation
   - Stereotyping or mocking cultural, ethnic, religious, gender, or linguistic identity
   - Sarcasm or humor that normalizes hostility
   - Implicit threats or glorification of harm
   - Dogwhistles, coded language, or community-specific slurs
   - Visual symbolism reinforcing discrimination (even without explicit slurs)
   - Juxtaposition of image + text that creates hateful meaning
   - Targeted abuse framed as "jokes", "facts", or "opinions"
   - Cyberbullying: personal attacks, body shaming, social exclusion, intimidation
   - Content making anyone feel unwelcome, inferior, or threatened

3. MULTIMODAL INTERPRETATION IS MANDATORY
   - Do NOT judge text or image independently.
   - The final meaning emerges from the *combination* of visual context, OCR text, and implied intent.
   - If the image amplifies or reframes neutral text into a hateful message, classify as HATE.

4. CULTURAL & LINGUISTIC AWARENESS
   - Pay special attention to:
     • Hindi-English and Bangla-English code-mixing
     • Local slang, memes, political references
     • Regional stereotypes and historical context
   - Treat translated meaning and implied sentiment as equally important.

5. SARCASM & IMPLICIT HATE HANDLING
   - Sarcasm does NOT reduce harm.
   - If sarcasm reinforces prejudice or exclusion, treat it as HATE.
   - Do not assume benign intent unless explicitly clear.

=====================================
REASONING PROCESS (INTERNAL ONLY)
=====================================

Internally reason step-by-step through:
1. What is shown visually?
2. What is explicitly stated in text?
3. What is implied when image and text are combined?
4. Who is the target?
5. Is harm, exclusion, or demeaning portrayal present?

Do NOT expose your chain-of-thought in the output.

=====================================
OUTPUT FORMAT (STRICT)
=====================================

Return a JSON object only:

{"label": "HATE" or "NON-HATE", "confidence": "LOW" or "MEDIUM" or "HIGH", "justification": "Concise explanation referencing visual-text interaction, cultural context, and inferred intent."}

=====================================
IMPORTANT REMINDERS
=====================================
- Do NOT default to NON-HATE to be safe.
- Do NOT apply moral neutrality.
- The goal is *sensitive detection*, not legal minimalism.
- Missing subtle hate is worse than a small number of false positives."""


# User prompt template for zero-shot - RECALL-OPTIMIZED with VLM signal
ZERO_SHOT_PROMPT_TEMPLATE = """Analyze the following meme for potential hate speech content.

=====================================
MEME ANALYSIS (FROM VLM)
=====================================

{vlm_context}

=====================================
CLASSIFICATION RULES
=====================================

**CRITICAL: Use the VLM Hate Risk Assessment as your primary guide:**
- If VLM Risk = HIGH → Default to HATE unless clearly a false alarm
- If VLM Risk = MEDIUM → Lean toward HATE if any doubt exists
- If VLM Risk = LOW → Only classify as NON-HATE if genuinely benign

Remember:
1. Sarcasm mocking groups = HATE
2. Stereotypes presented as humor = HATE  
3. Personal attacks/cyberbullying = HATE
4. Content that makes groups feel unwelcome = HATE

When uncertain, classify as HATE. Missing hate is worse than false alarms.

=====================================
YOUR RESPONSE
=====================================

Respond with JSON only:
{{"label": "HATE" or "NON-HATE", "confidence": "LOW/MEDIUM/HIGH", "justification": "..."}}"""


class ZeroShotClassifier(BaseClassifier):
    """
    Zero-shot hate speech classifier.
    
    Uses only the task description without examples to classify content.
    This tests the model's inherent understanding of hate speech.
    """
    
    def __init__(self, llm: BaseLLM = None):
        """
        Initialize the zero-shot classifier.
        
        Args:
            llm: The LLM to use for reasoning. Creates default if None.
        """
        if llm is None:
            llm = OpenRouterLLM()
        super().__init__(llm)
    
    @property
    def mode_name(self) -> str:
        """Return the mode name."""
        return "zero_shot"
    
    async def classify(self, vlm_output: VLMOutput) -> ClassificationResult:
        """
        Classify meme content using zero-shot inference.
        
        Args:
            vlm_output: Structured output from VLM image analysis.
            
        Returns:
            ClassificationResult with label and justification.
        """
        # Format VLM output for prompt
        vlm_context = vlm_output.to_context_string()
        
        # Create prompt
        prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(vlm_context=vlm_context)
        
        # Get LLM response
        response = await self.llm.complete(
            prompt=prompt,
            system_prompt=ZERO_SHOT_SYSTEM_PROMPT
        )
        
        # Parse response
        try:
            parsed = OpenRouterLLM.parse_json_from_response(response)
            
            label = parsed.get("label", "").upper()
            if label not in ["HATE", "NON-HATE"]:
                # Default to HATE if unclear (err on side of caution)
                label = "HATE"
            
            justification = parsed.get("justification", "Unable to parse justification")
            
            # Parse confidence level (new field from recall-optimized prompt)
            confidence_level = parsed.get("confidence", "").upper()
            if confidence_level not in ["LOW", "MEDIUM", "HIGH"]:
                confidence_level = None
            
            return ClassificationResult(
                label=label,
                justification=justification,
                confidence_level=confidence_level
            )
            
        except ValueError as e:
            # If parsing fails, try to extract label from text
            response_upper = response.upper()
            if "NON-HATE" in response_upper:
                label = "NON-HATE"
            else:
                # Default to HATE when uncertain
                label = "HATE"
            
            return ClassificationResult(
                label=label,
                justification=f"Classification based on response pattern. Original: {response[:100]}..."
            )
